{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original NNM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import yaml\n",
    "from termcolor import colored\n",
    "from utils.common_config import get_val_dataset, get_val_transformations, get_val_dataloader,\\\n",
    "                                get_model\n",
    "from utils.evaluate_utils import hungarian_evaluate\n",
    "from utils.memory import MemoryBank \n",
    "from utils.utils import fill_memory_bank\n",
    "from PIL import Image\n",
    "\n",
    "from data.custom_dataset import NeighborsDataset\n",
    "from utils.common_config import get_feature_dimensions_backbone\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_prototypes(config, predictions, features, model, topk=3):\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # Get topk most certain indices and pred labels\n",
    "    print('Get topk')\n",
    "    probs = predictions['probabilities']\n",
    "    n_classes = probs.shape[1]\n",
    "    dims = features.shape[1]\n",
    "    max_probs, pred_labels = torch.max(probs, dim = 1)\n",
    "    indices = torch.zeros((n_classes, topk))\n",
    "    for pred_id in range(n_classes):\n",
    "        probs_copy = max_probs.clone()\n",
    "        mask_out = ~(pred_labels == pred_id)\n",
    "        probs_copy[mask_out] = -1\n",
    "        conf_vals, conf_idx = torch.topk(probs_copy, k = topk, largest = True, sorted = True)\n",
    "        indices[pred_id, :] = conf_idx\n",
    "\n",
    "    # # Get corresponding features\n",
    "    # selected_features = torch.index_select(features, dim=0, index=indices.view(-1).long())\n",
    "    # selected_features = selected_features.unsqueeze(1).view(n_classes, -1, dims)\n",
    "\n",
    "    # # Get mean feature per class\n",
    "    # mean_features = torch.mean(selected_features, dim=1)\n",
    "\n",
    "    # # Get min distance wrt to mean\n",
    "    # diff_features = selected_features - mean_features.unsqueeze(1)\n",
    "    # diff_norm = torch.norm(diff_features, 2, dim=2)\n",
    "\n",
    "    # # Get final indices\n",
    "    # _, best_indices = torch.min(diff_norm, dim=1)\n",
    "    # one_hot = F.one_hot(best_indices.long(), indices.size(1)).byte()\n",
    "    # proto_indices = torch.masked_select(indices.view(-1), one_hot.view(-1))\n",
    "    # proto_indices = proto_indices.int().tolist()\n",
    "    # return proto_indices\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_indices(indices, dataset, hungarian_match, config):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # for idx in indices:\n",
    "    #     img = np.array(dataset.get_image(idx)).astype(np.uint8)\n",
    "    #     img = Image.fromarray(img)\n",
    "    #     plt.figure()\n",
    "    #     plt.axis('off')\n",
    "    #     plt.imshow(img)\n",
    "    #     #plt.show()\n",
    "    #     plt.savefig(config['train_db_name']+'_'+str(idx)+'_.png', dpi=500, bbox_inches='tight')\n",
    "\n",
    "    for c in range(indices.shape[0]) :\n",
    "        indices_c = indices[c,:].int().tolist()\n",
    "        for idx in indices_c:\n",
    "            img = np.array(dataset.get_image(idx)).astype(np.uint8)\n",
    "            img = Image.fromarray(img)\n",
    "            plt.figure()\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img)\n",
    "            #plt.show()\n",
    "            plt.savefig('./'+config['train_db_name']+'/'+config['train_db_name']+'_'+str(c)+'_'+str(idx)+'.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_predictions(p, dataloader, model, return_features=False):\n",
    "    # Make predictions on a dataset with neighbors\n",
    "    model.eval()\n",
    "    predictions = [[] for _ in range(p['num_heads'])]\n",
    "    probs = [[] for _ in range(p['num_heads'])]\n",
    "    targets = []\n",
    "    if return_features:\n",
    "        ft_dim = get_feature_dimensions_backbone(p)\n",
    "        features = torch.zeros((len(dataloader.sampler), ft_dim)).cuda()\n",
    "    \n",
    "    if isinstance(dataloader.dataset, NeighborsDataset): # Also return the neighbors\n",
    "        key_ = 'anchor'\n",
    "        include_neighbors = True\n",
    "        neighbors = []\n",
    "\n",
    "    else:\n",
    "        key_ = 'image'\n",
    "        include_neighbors = False\n",
    "\n",
    "    ptr = 0\n",
    "    for batch in dataloader:\n",
    "        images = batch[key_].cuda(non_blocking=True)\n",
    "        bs = images.shape[0]\n",
    "        res = model(images, forward_pass='return_all')\n",
    "        output = res['output']\n",
    "        if return_features:\n",
    "            features[ptr: ptr+bs] = res['features']\n",
    "            ptr += bs\n",
    "        for i, output_i in enumerate(output):\n",
    "            predictions[i].append(torch.argmax(output_i, dim=1))\n",
    "            probs[i].append(F.softmax(output_i, dim=1))\n",
    "        targets.append(batch['target'])\n",
    "        if include_neighbors:\n",
    "            neighbors.append(batch['possible_neighbors'])\n",
    "\n",
    "    predictions = [torch.cat(pred_, dim = 0).cpu() for pred_ in predictions]\n",
    "    probs = [torch.cat(prob_, dim=0).cpu() for prob_ in probs]\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    if include_neighbors:\n",
    "        neighbors = torch.cat(neighbors, dim=0)\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets, 'neighbors': neighbors} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    else:\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    if return_features:\n",
    "        return out, features\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = argparse.ArgumentParser(description='Evaluate models from the model zoo')\n",
    "FLAGS.add_argument('--config_exp', help='Location of config file', default='configs/scan/scan_cifar20.yml')\n",
    "FLAGS.add_argument('--model', help='Location where model is saved', default=\"cifar20.pth.tar\")\n",
    "FLAGS.add_argument('--visualize_prototypes', action='store_true', \n",
    "                    help='Show the prototpye for each cluster', default=False)\n",
    "args = FLAGS.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config file\n",
    "print(colored('Read config file {} ...'.format(args.config_exp), 'blue'))\n",
    "with open(args.config_exp, 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "config['batch_size'] = 256 # To make sure we can evaluate on a single 1080ti\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "print(colored('Get validation dataset ...', 'blue'))\n",
    "transforms = get_val_transformations(config)\n",
    "dataset = get_val_dataset(config, transforms)\n",
    "dataloader = get_val_dataloader(config, dataset)\n",
    "print('Number of samples: {}'.format(len(dataset)))\n",
    "\n",
    "# Get model\n",
    "print(colored('Get model ...', 'blue'))\n",
    "model = get_model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model weights\n",
    "print(colored('Load model weights ...', 'blue'))\n",
    "state_dict = torch.load(args.model, map_location='cpu')\n",
    "\n",
    "if config['setup'] in ['simclr', 'moco', 'selflabel']:\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "elif config['setup'] == 'scan':\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['setup'] in ['scan', 'selflabel']:\n",
    "    print(colored('Perform evaluation of the clustering model (setup={}).'.format(config['setup']), 'blue'))\n",
    "    head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "    predictions, features = get_predictions(config, dataloader, model, return_features=True)\n",
    "    clustering_stats = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                            compute_confusion_matrix=True, \n",
    "                        confusion_matrix_file=None)\n",
    "    print(clustering_stats)\n",
    "    if args.visualize_prototypes:\n",
    "        prototype_indices = get_prototypes(config, predictions[head], features, model)\n",
    "        visualize_indices(prototype_indices, dataset, clustering_stats['hungarian_match'], config)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gan_attack\n",
    "importlib.reload(gan_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=True\n",
    "image_nc=3\n",
    "epochs = 60\n",
    "batch_size = 200\n",
    "BOX_MIN = 0\n",
    "BOX_MAX = 1\n",
    "model_num_labels = 20\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN = gan_attack.GAN_Attack(device, model, model_num_labels, image_nc, BOX_MIN, BOX_MAX, 'cifar100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Holding the original output object. i.e. console out\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "# Opening the file to write file deletion logs.\n",
    "f = open('outgan_train_c100256.txt', 'a+')\n",
    "\n",
    "# Changing standard out to file out. \n",
    "sys.stdout = f\n",
    "# This will write to the file. \n",
    "print(\"xyz\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "advGAN.train(dataloader, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the file.\n",
    "f.close()\n",
    "\n",
    "# replacing the original output format to stdout.\n",
    "sys.stdout = orig_stdout\n",
    "\n",
    "# This will print onto the console.\n",
    "print(\"xyz\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial NMI, ARI, F, and ACC\n",
    "device = 'cuda'\n",
    "import models_clu\n",
    "use_cuda=True\n",
    "image_nc=3\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "# load the generator of adversarial examples\n",
    "# pretrained_generator_path = './models/netG_cc_epoch_120.pth'\n",
    "pretrained_generator_path = './models/netG_cc_cifar100_epoch_570.pth'\n",
    "pretrained_G = models_clu.Generator(gen_input_nc, image_nc).to(device)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
    "pretrained_G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "def plot_examples(images, labels):\n",
    "    print(type(images[0]), type(labels))\n",
    "    print(images.shape)\n",
    "    w = 10\n",
    "    h = 10\n",
    "    fig = plt.figure(figsize=(10, 20))\n",
    "    columns = 11\n",
    "    rows = 12\n",
    "    for i in range(10):\n",
    "#         img = np.random.randint(10, size=(h,w))\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "#         img = images[i] / 2 + 0.5   # unnormalize\n",
    "        img = images[i]\n",
    "        npimg = img.detach().cpu().numpy()   # convert from tensor\n",
    "        \n",
    "#         plt.imshow(np.transpose(npimg, (1, 2, 0))) \n",
    "#         plt.imshow(npimg)\n",
    "#   plt.show()\n",
    "        plt.imshow(np.transpose(np.reshape(images[i].detach().cpu(), (3, 32,32)), (1,2,0)))\n",
    "        plt.title('#{}: {}'.format(i, labels[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'mean': [0.5071, 0.4867, 0.4408], 'std': [0.2675, 0.2565, 0.2761]\n",
    "\n",
    "\n",
    "def save_examples(images, labels, noise=False, bno=0, adv=False, orig=False):\n",
    "    print(type(images[0]), type(labels))\n",
    "#     MEAN = torch.tensor([0.4914, 0.4822, 0.4465]).cuda() #c10\n",
    "#     STD = torch.tensor([0.2023, 0.1994, 0.2010]).cuda() #c10\n",
    "#     MEAN = torch.tensor([0.5071, 0.4867, 0.4408]).cuda()\n",
    "#     STD = torch.tensor([0.2675, 0.2565, 0.2761]).cuda()\n",
    "    MEAN = torch.tensor([0.5071, 0.4867, 0.4408]).cuda() #c100\n",
    "    STD = torch.tensor([0.2675, 0.2565, 0.2761]).cuda() #c100\n",
    "    \n",
    "    for i in range(min(len(images), 20)):\n",
    "        img = images[i]\n",
    "        img = img * STD[:, None, None] + MEAN[:, None, None]\n",
    "        npimg = img.detach().cpu().numpy()   # convert from tensor\n",
    "        npimg = np.clip(npimg, 0, 1)\n",
    "        if orig:\n",
    "#             npimg = np.transpose(npimg, (1, 2, 0))\n",
    "            plt.imsave(f'../Images/C100/NNM/orig/NNM_c100_b{bno}_{i}_lab{labels[i]:02}.png', npimg.T, dpi=600)\n",
    "            continue\n",
    "        if adv:\n",
    "#             npimg = np.transpose(npimg, (1, 2, 0))\n",
    "            plt.imsave(f'../Images/C100/NNM/adv/NNM_c100_b{bno}_{i}_lab{labels[i]:02}.png', npimg.T, dpi=600)\n",
    "            continue\n",
    "        if noise:\n",
    "            npimg = npimg / 2 + 0.5 \n",
    "            plt.imsave(f'../Images/C100/NNM/noise/NNM_c100_b{bno}_{i}_noise_lab{labels[i]:02}.png', npimg.T, dpi=600)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_predictions_adv(p, dataloader, model, return_features=False):\n",
    "    # Make predictions on a dataset with neighbors\n",
    "    model.eval()\n",
    "    predictions = [[] for _ in range(p['num_heads'])]\n",
    "    probs = [[] for _ in range(p['num_heads'])]\n",
    "    targets = []\n",
    "    if return_features:\n",
    "        ft_dim = get_feature_dimensions_backbone(p)\n",
    "        features = torch.zeros((len(dataloader.sampler), ft_dim)).cuda()\n",
    "    \n",
    "    if isinstance(dataloader.dataset, NeighborsDataset): # Also return the neighbors\n",
    "        key_ = 'anchor'\n",
    "        include_neighbors = True\n",
    "        neighbors = []\n",
    "\n",
    "    else:\n",
    "        key_ = 'image'\n",
    "        include_neighbors = False\n",
    "\n",
    "    ptr = 0\n",
    "    fla = True\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        images = batch['image']\n",
    "        labels = batch['target']\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bs = images.shape[0]\n",
    "        perturbation = pretrained_G(images)\n",
    "        perturbation = torch.clamp(perturbation, -0.15, 0.15)\n",
    "        adv_imgs = perturbation + images\n",
    "#         adv_imgs = torch.clamp(adv_imgs, 0, 1)\n",
    "        \n",
    "        if fla:\n",
    "            fla = False\n",
    "            plot_examples(images, [0]*len(adv_imgs))\n",
    "            plot_examples(adv_imgs, [0]*len(adv_imgs))\n",
    "        \n",
    "        res = model(adv_imgs, forward_pass='return_all')\n",
    "        output = res['output']\n",
    "\n",
    "        if return_features:\n",
    "            features[ptr: ptr+bs] = res['features']\n",
    "            ptr += bs\n",
    "        for i, output_i in enumerate(output):\n",
    "            predictions[i].append(torch.argmax(output_i, dim=1))\n",
    "            probs[i].append(F.softmax(output_i, dim=1))\n",
    "        targets.append(labels)\n",
    "        if include_neighbors:\n",
    "            neighbors.append(batch['possible_neighbors'])\n",
    "\n",
    "    predictions = [torch.cat(pred_, dim = 0).cpu() for pred_ in predictions]\n",
    "    probs = [torch.cat(prob_, dim=0).cpu() for prob_ in probs]\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    if include_neighbors:\n",
    "        neighbors = torch.cat(neighbors, dim=0)\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets, 'neighbors': neighbors} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    else:\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    if return_features:\n",
    "        return out, features\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_predictions_adv_save(p, dataloader, model, return_features=False):\n",
    "    # Make predictions on a dataset with neighbors\n",
    "    model.eval()\n",
    "    predictions = [[] for _ in range(p['num_heads'])]\n",
    "    probs = [[] for _ in range(p['num_heads'])]\n",
    "    targets = []\n",
    "    if return_features:\n",
    "        ft_dim = get_feature_dimensions_backbone(p)\n",
    "        features = torch.zeros((len(dataloader.sampler), ft_dim)).cuda()\n",
    "    \n",
    "    if isinstance(dataloader.dataset, NeighborsDataset): # Also return the neighbors\n",
    "        key_ = 'anchor'\n",
    "        include_neighbors = True\n",
    "        neighbors = []\n",
    "\n",
    "    else:\n",
    "        key_ = 'image'\n",
    "        include_neighbors = False\n",
    "\n",
    "    ptr = 0\n",
    "    fla = True\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        images = batch['image']\n",
    "        labels = batch['target']\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bs = images.shape[0]\n",
    "        perturbation = pretrained_G(images)\n",
    "        perturbation = torch.clamp(perturbation, -0.15, 0.15)\n",
    "        adv_imgs = perturbation + images\n",
    "#         adv_imgs = torch.clamp(adv_imgs, 0, 1)\n",
    "        \n",
    "        if fla:\n",
    "            fla = False\n",
    "            plot_examples(images, [0]*len(adv_imgs))\n",
    "            plot_examples(adv_imgs, [0]*len(adv_imgs))\n",
    "        \n",
    "        res = model(adv_imgs, forward_pass='return_all')\n",
    "        output = res['output']\n",
    "\n",
    "        if return_features:\n",
    "            features[ptr: ptr+bs] = res['features']\n",
    "            ptr += bs\n",
    "        for i, output_i in enumerate(output):\n",
    "            predictions[i].append(torch.argmax(output_i, dim=1))\n",
    "            probs[i].append(F.softmax(output_i, dim=1))\n",
    "        targets.append(labels)\n",
    "        save_examples(images, labels.data.cpu().numpy(), bno=step, orig=True)\n",
    "        save_examples(adv_imgs, predictions[-1][0].cpu().numpy(), bno=step, adv=True)\n",
    "        save_examples(perturbation, labels.data.cpu().numpy(), bno=step, noise=True)\n",
    "        if include_neighbors:\n",
    "            neighbors.append(batch['possible_neighbors'])\n",
    "\n",
    "    predictions = [torch.cat(pred_, dim = 0).cpu() for pred_ in predictions]\n",
    "    probs = [torch.cat(prob_, dim=0).cpu() for prob_ in probs]\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    if include_neighbors:\n",
    "        neighbors = torch.cat(neighbors, dim=0)\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets, 'neighbors': neighbors} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    else:\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    if return_features:\n",
    "        return out, features\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving samples\n",
    "# predictions, features = get_predictions_adv_save(config, dataloader, model, return_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['setup'] in ['scan', 'selflabel']:\n",
    "        print(colored('Perform evaluation of the clustering model (setup={}).'.format(config['setup']), 'blue'))\n",
    "        head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "        predictions, features = get_predictions_adv(config, dataloader, model, return_features=True)\n",
    "        clustering_stats_adv = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                                compute_confusion_matrix=True, \n",
    "                            confusion_matrix_file=None)\n",
    "        print(clustering_stats)\n",
    "        print(clustering_stats_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial NMI, ARI, F, and ACC\n",
    "device = 'cuda'\n",
    "import models_clu\n",
    "use_cuda=True\n",
    "image_nc=3\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "# load the generator of adversarial examples\n",
    "# pretrained_generator_path = './models/netG_cc_epoch_120.pth'\n",
    "pretrained_generator_path = '../Generator_Models/CIFAR100/netG_cc_CIFAR-100_epoch_120.pth'\n",
    "pretrained_G = models_clu.Generator(gen_input_nc, image_nc).to(device)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
    "pretrained_G.eval()\n",
    "\n",
    "if config['setup'] in ['scan', 'selflabel']:\n",
    "        print(colored('Perform evaluation of the clustering model (setup={}).'.format(config['setup']), 'blue'))\n",
    "        head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "        predictions, features = get_predictions_adv(config, dataloader, model, return_features=True)\n",
    "        clustering_stats_adv = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                                compute_confusion_matrix=True, \n",
    "                            confusion_matrix_file=None)\n",
    "        print(clustering_stats)\n",
    "        print(clustering_stats_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial NMI, ARI, F, and ACC\n",
    "device = 'cuda'\n",
    "import models_clu\n",
    "use_cuda=True\n",
    "image_nc=3\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "# load the generator of adversarial examples\n",
    "# pretrained_generator_path = './models/netG_cc_epoch_120.pth'\n",
    "pretrained_generator_path = '../Generator_Models/CIFAR100/netG_SCAN_CIFAR20.pth'\n",
    "pretrained_G = models_clu.Generator(gen_input_nc, image_nc).to(device)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
    "pretrained_G.eval()\n",
    "\n",
    "if config['setup'] in ['scan', 'selflabel']:\n",
    "        print(colored('Perform evaluation of the clustering model (setup={}).'.format(config['setup']), 'blue'))\n",
    "        head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "        predictions, features = get_predictions_adv(config, dataloader, model, return_features=True)\n",
    "        clustering_stats_adv = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                                compute_confusion_matrix=True, \n",
    "                            confusion_matrix_file=None)\n",
    "        print(clustering_stats)\n",
    "        print(clustering_stats_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial NMI, ARI, F, and ACC\n",
    "device = 'cuda'\n",
    "import models_clu\n",
    "use_cuda=True\n",
    "image_nc=3\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "# load the generator of adversarial examples\n",
    "# pretrained_generator_path = './models/netG_cc_epoch_120.pth'\n",
    "pretrained_generator_path = '../Generator_Models/CIFAR100/netG_RUC_CIFAR20.pth'\n",
    "pretrained_G = models_clu.Generator(gen_input_nc, image_nc).to(device)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
    "pretrained_G.eval()\n",
    "\n",
    "if config['setup'] in ['scan', 'selflabel']:\n",
    "        print(colored('Perform evaluation of the clustering model (setup={}).'.format(config['setup']), 'blue'))\n",
    "        head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "        predictions, features = get_predictions_adv(config, dataloader, model, return_features=True)\n",
    "        clustering_stats_adv = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                                compute_confusion_matrix=True, \n",
    "                            confusion_matrix_file=None)\n",
    "        print(clustering_stats)\n",
    "        print(clustering_stats_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial NMI, ARI, F, and ACC\n",
    "device = 'cuda'\n",
    "import models_clu\n",
    "use_cuda=True\n",
    "image_nc=3\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "# load the generator of adversarial examples\n",
    "# pretrained_generator_path = './models/netG_cc_epoch_120.pth'\n",
    "pretrained_generator_path = '../Generator_Models/CIFAR100/netG_SPICE_CIFAR20.pth'\n",
    "pretrained_G = models_clu.Generator(gen_input_nc, image_nc).to(device)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
    "pretrained_G.eval()\n",
    "\n",
    "if config['setup'] in ['scan', 'selflabel']:\n",
    "        print(colored('Perform evaluation of the clustering model (setup={}).'.format(config['setup']), 'blue'))\n",
    "        head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "        predictions, features = get_predictions_adv(config, dataloader, model, return_features=True)\n",
    "        clustering_stats_adv = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                                compute_confusion_matrix=True, \n",
    "                            confusion_matrix_file=None)\n",
    "        print(clustering_stats)\n",
    "        print(clustering_stats_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial NMI, ARI, F, and ACC\n",
    "device = 'cuda'\n",
    "import models_clu\n",
    "use_cuda=True\n",
    "image_nc=3\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "# load the generator of adversarial examples\n",
    "# pretrained_generator_path = './models/netG_cc_epoch_120.pth'\n",
    "pretrained_generator_path = '../Generator_Models/CIFAR100/netG_MICE_CIFAR20.pth'\n",
    "pretrained_G = models_clu.Generator(gen_input_nc, image_nc).to(device)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
    "pretrained_G.eval()\n",
    "\n",
    "if config['setup'] in ['scan', 'selflabel']:\n",
    "        print(colored('Perform evaluation of the clustering model (setup={}).'.format(config['setup']), 'blue'))\n",
    "        head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "        predictions, features = get_predictions_adv(config, dataloader, model, return_features=True)\n",
    "        clustering_stats_adv = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                                compute_confusion_matrix=True, \n",
    "                            confusion_matrix_file=None)\n",
    "        print(clustering_stats)\n",
    "        print(clustering_stats_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Experiemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_predictions_adv_norm(p, dataloader, model, return_features=False, clamping=0.1):\n",
    "    # Make predictions on a dataset with neighbors\n",
    "    model.eval()\n",
    "    predictions = [[] for _ in range(p['num_heads'])]\n",
    "    probs = [[] for _ in range(p['num_heads'])]\n",
    "    targets = []\n",
    "    if return_features:\n",
    "        ft_dim = get_feature_dimensions_backbone(p)\n",
    "        features = torch.zeros((len(dataloader.sampler), ft_dim)).cuda()\n",
    "    \n",
    "    if isinstance(dataloader.dataset, NeighborsDataset): # Also return the neighbors\n",
    "        key_ = 'anchor'\n",
    "        include_neighbors = True\n",
    "        neighbors = []\n",
    "\n",
    "    else:\n",
    "        key_ = 'image'\n",
    "        include_neighbors = False\n",
    "\n",
    "    ptr = 0\n",
    "    fla = True\n",
    "    norm = 0.0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        images = batch['image']\n",
    "        labels = batch['target']\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "        bs = images.shape[0]\n",
    "        perturbation = pretrained_G(images)\n",
    "        perturbation = torch.clamp(perturbation, -clamping, clamping)\n",
    "        norm += torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1)).to('cpu').item()\n",
    "        adv_imgs = perturbation + images\n",
    "#         adv_imgs = torch.clamp(adv_imgs, 0, 1)\n",
    "        \n",
    "        res = model(adv_imgs, forward_pass='return_all')\n",
    "        output = res['output']\n",
    "\n",
    "        if return_features:\n",
    "            features[ptr: ptr+bs] = res['features']\n",
    "            ptr += bs\n",
    "        for i, output_i in enumerate(output):\n",
    "            predictions[i].append(torch.argmax(output_i, dim=1))\n",
    "            probs[i].append(F.softmax(output_i, dim=1))\n",
    "        targets.append(labels)\n",
    "        if include_neighbors:\n",
    "            neighbors.append(batch['possible_neighbors'])\n",
    "\n",
    "    predictions = [torch.cat(pred_, dim = 0).cpu() for pred_ in predictions]\n",
    "    probs = [torch.cat(prob_, dim=0).cpu() for prob_ in probs]\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    if include_neighbors:\n",
    "        neighbors = torch.cat(neighbors, dim=0)\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets, 'neighbors': neighbors} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    else:\n",
    "        out = [{'predictions': pred_, 'probabilities': prob_, 'targets': targets} for pred_, prob_ in zip(predictions, probs)]\n",
    "\n",
    "    if return_features:\n",
    "        return out, features, norm/len(dataloader)\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adversarial NMI, ARI, F, and ACC\n",
    "device = 'cuda'\n",
    "import models_clu\n",
    "use_cuda=True\n",
    "image_nc=3\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "# load the generator of adversarial examples\n",
    "# pretrained_generator_path = './models/netG_cc_epoch_120.pth'\n",
    "pretrained_generator_path = './models/netG_cc_cifar100_epoch_600.pth'\n",
    "pretrained_G = models_clu.Generator(gen_input_nc, image_nc).to(device)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))\n",
    "pretrained_G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_l = []\n",
    "nmi_l = []\n",
    "ari_l = []\n",
    "acc_l = []\n",
    "# clamp = [j for j in range(0, 1, 0.02)]\n",
    "# clamp = [j for j in np.arange(0, 1.05, 0.05)]\n",
    "# clamp = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "clamp = [0, 0.001, 0.003, 0.005, 0.007, 0.01, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.30, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "print(clamp)\n",
    "\n",
    "for j in clamp:\n",
    "    torch.cuda.empty_cache()\n",
    "#     acc2, mice_pi_acc, nmi, ari, norm = get_MiCE_adv_performance_norm(model, model_ema, elbo, full_loader, n_data, n_class, clamping=j)\n",
    "    predictions, features, norm = get_predictions_adv_norm(config, dataloader, model, return_features=True, clamping=j)\n",
    "    head = state_dict['head'] if config['setup'] == 'scan' else 0\n",
    "    \n",
    "    clustering_stats_adv = hungarian_evaluate(head, predictions, dataset.classes, \n",
    "                                            compute_confusion_matrix=True, \n",
    "                        confusion_matrix_file=None)\n",
    "#         print(clustering_stats)\n",
    "    nmi = clustering_stats_adv['NMI']\n",
    "    ari = clustering_stats_adv['ARI']\n",
    "    acc = clustering_stats_adv['ACC']\n",
    "    \n",
    "    print(f'clamp {j} avg norm: {norm}')\n",
    "    print('NMI = {:.4f} ARI = {:.4f} ACC = {:.4f}'.format(nmi, ari, acc))\n",
    "    norm_l.append(norm)\n",
    "    nmi_l.append(nmi)\n",
    "    ari_l.append(ari)\n",
    "    acc_l.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x, y, label = \"line 1\", linestyle=\"-\")\n",
    "# plt.plot(y, x, label = \"line 2\", linestyle=\"--\")\n",
    "# plt.plot(x, np.sin(x), label = \"curve 1\", linestyle=\"-.\")\n",
    "# plt.plot(x, np.cos(x), label = \"curve 2\", linestyle=\":\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "plt.plot(norm_l, nmi_l, label = \"nmi\", linestyle=\"-\")\n",
    "plt.plot(norm_l, ari_l, label = \"ari\", linestyle=\"-\")\n",
    "plt.plot(norm_l, acc_l, label = \"acc\", linestyle=\"-\")\n",
    "plt.xlabel(\"Perturbation Norm\")\n",
    "plt.ylabel(\"Performace\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('nnm_cifar10.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_l)\n",
    "print()\n",
    "print(nmi_l)\n",
    "print()\n",
    "print(ari_l)\n",
    "print()\n",
    "print(acc_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval_cus\n",
    "\n",
    "norm_l = []\n",
    "nmi_l = []\n",
    "ari_l = []\n",
    "acc_l = []\n",
    "# clamp = [j for j in range(0, 1, 0.02)]\n",
    "# clamp = [j for j in np.arange(0, 1.05, 0.05)]\n",
    "# clamp = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "# clamp = [0, 0.001, 0.003, 0.005, 0.007, 0.01, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.30, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "clamp = [0, 0.1, 1]\n",
    "print(clamp)\n",
    "\n",
    "for j in clamp:\n",
    "    torch.cuda.empty_cache()\n",
    "#     acc2, mice_pi_acc, nmi, ari, norm = get_MiCE_adv_performance_norm(model, model_ema, elbo, full_loader, n_data, n_class, clamping=j)\n",
    "    predictions, features, norm = get_predictions_adv_norm(config, dataloader, model, return_features=True, clamping=j)\n",
    "\n",
    "    out = predictions[0]['predictions']\n",
    "    targets = predictions[0]['targets']\n",
    "    clustering_stats_adv = eval_cus.check(targets, out, 20, dataset.classes, \n",
    "                                            compute_confusion_matrix=True, \n",
    "                        confusion_matrix_file=None, cf20=True, output_file2=f'NNM_c100_{j}_n{norm}.pdf')\n",
    "    #         print(clustering_stats)\n",
    "    nmi = clustering_stats_adv['NMI']\n",
    "    ari = clustering_stats_adv['ARI']\n",
    "    acc = clustering_stats_adv['ACC']\n",
    "    \n",
    "    print(f'clamp {j} avg norm: {norm}')\n",
    "    print('NMI = {:.4f} ARI = {:.4f} ACC = {:.4f}'.format(nmi, ari, acc))\n",
    "    norm_l.append(norm)\n",
    "    nmi_l.append(nmi)\n",
    "    ari_l.append(ari)\n",
    "    acc_l.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, features, norm = get_predictions_adv_norm(config, dataloader, model, return_features=True, clamping=0.1)\n",
    "\n",
    "out = predictions[0]['predictions']\n",
    "targets = predictions[0]['targets']\n",
    "clustering_stats_adv = eval_cus.check(targets, out, 20, dataset.classes, \n",
    "                                        compute_confusion_matrix=True, \n",
    "                    confusion_matrix_file=None, cf20=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustering_stats_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_l)\n",
    "print()\n",
    "print(nmi_l)\n",
    "print()\n",
    "print(ari_l)\n",
    "print()\n",
    "print(acc_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
